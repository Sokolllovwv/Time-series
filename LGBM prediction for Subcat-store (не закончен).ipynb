{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cae7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7964a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea2c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2222f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "#def merge_by_concat(df1, df2, merge_on):\n",
    "#    merged_gf = df1[merge_on]\n",
    "#    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "#    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "#    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "#    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5207524",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "TARGET = 'fact'         # Our main target\n",
    "END_TRAIN = '20230108'         # Last day in train set\n",
    "MAIN_INDEX = ['id','calday']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e947678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_ = r'C:/Users/vadim.sokolov/Desktop/В/LGBM prediction for Cannibalization/A1 First place/' # input only here\n",
    "raw_data_dir = dir_+'2. data/'\n",
    "processed_data_dir = dir_+'2. data/processed/'\n",
    "\n",
    "df = pd.read_csv(raw_data_dir + 'LGBM_test_3tk_2pdk.csv', sep = ',')\n",
    "holidays = pd.read_csv(raw_data_dir + 'Holidays_2022_2023.csv', sep = ';')\n",
    "\n",
    "df = pd.merge(df, holidays, on = ['calday'], how = 'left').fillna('na')\n",
    "\n",
    "df['calday_time'] = df['calday'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "df['year_iso'] = pd.to_numeric(df['year_iso'])\n",
    "df['day'] = df['calday_time'].dt.day.astype(np.int8)\n",
    "df['dayofweek'] = df['calday_time'].dt.dayofweek.astype(np.int8)\n",
    "df['month'] = df['calday_time'].dt.month.astype(np.int8)\n",
    "df['rt_promoct_unit_ABC'] = df['rt_promoct_corr'].map(str) + '-' + df['unit'].map(str) + '-' + df['ABC'].map(str)\n",
    "\n",
    "df['productlist'] = df['plant'].map(str) + '-' + df['subcat_id'].map(str) + '-' + df['rt_promoct_corr'].map(str)\n",
    "productlist = df.loc[(df['calday'] >= 20230109), 'productlist']\n",
    "productlist = list(productlist.unique())\n",
    "df = df[df['productlist'].isin(productlist)]\n",
    "df = df.drop(columns=['productlist','unit','rt_promoct_corr','ABC'])\n",
    "\n",
    "df['id'] = df['plant'].map(str) + '-' + df['subcat_id'].map(str) + '-' + df['rt_promoct_unit_ABC'].map(str) \n",
    "df = df.sort_values(['id','calday'],ascending=True)\n",
    "df = df[['year_iso', 'month', 'week_iso', 'calday', 'day', 'dayofweek', 'calday_time', 'Holiday_type', \n",
    "        'region_num', 'format', 'plant', 'group_id', 'cat_id', 'subcat_id', 'rt_promoct_unit_ABC', 'id', \n",
    "        'flag_oos',  'duration_rt_promoct_days_in_week_max', 'day_sales_qty_total', 'TotalSales', 'avg_sales_day_sku_plant', \n",
    "        'price_total', 'fact']\n",
    "         ]\n",
    "len(df)\n",
    "\n",
    "#df['flag_oos'] = df['flag_oos'].apply(math.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a272f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year_iso', 'month', 'week_iso', 'calday', 'day', 'dayofweek',\n",
       "       'calday_time', 'Holiday_type', 'region_num', 'format', 'plant',\n",
       "       'group_id', 'cat_id', 'subcat_id', 'rt_promoct_unit_ABC', 'id',\n",
       "       'flag_oos', 'duration_rt_promoct_days_in_week_max',\n",
       "       'day_sales_qty_total', 'TotalSales', 'avg_sales_day_sku_plant',\n",
       "       'price_total', 'fact'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad16f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Original grid_df:   2.5MiB\n",
      "          Reduced df:   2.5MiB\n"
     ]
    }
   ],
   "source": [
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(df.memory_usage(index=True).sum())))\n",
    "\n",
    "# We can free some memory by converting \"strings\" to categorical it will not affect merging and we will not lose any value data.\n",
    "#index_columns = ['region_num','plant','group_id','cat_id','subcat_id','unit','rt_promoct_corr','format','ABC']\n",
    "#for col in index_columns:\n",
    "#    df[col] = df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced df',sizeof_fmt(df.memory_usage(index=True).sum())))\n",
    "\n",
    "df.to_pickle(processed_data_dir+'a. grid_part_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f76f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  1.21 Mb (51.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41399f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Merge calendar #################################################################################\n",
    "#grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "#icols = ['date','d','event_name_1','event_type_1','event_name_2','event_type_2', 'snap_CA','snap_TX','snap_WI']\n",
    "\n",
    "# Minify data 'snap_' columns we can convert to bool or int8\n",
    "#icols = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\n",
    "#for col in icols:\n",
    "#    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert to DateTime\n",
    "#df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Make some features from date\n",
    "\n",
    "#df['tm_w'] = ['date'].dt.week.astype(np.int8)\n",
    "#df['tm_y'] = ['date'].dt.year\n",
    "#df['tm_y'] = (['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "#df['tm_wm'] = ['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8) # 오늘 몇째주?\n",
    "\n",
    "#df['tm_dw'] = ['date'].dt.dayofweek.astype(np.int8) \n",
    "#df['tm_w_end'] = (['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "#del df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5697ff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create lags\n",
      "0.01 min: Lags\n",
      "Create rolling aggs\n",
      "Rolling period: 7\n",
      "Rolling period: 14\n",
      "Rolling period: 30\n",
      "Rolling period: 60\n",
      "Rolling period: 180\n",
      "Shifting period: 1\n",
      "Shifting period: 7\n",
      "Shifting period: 14\n",
      "0.03 min: Lags\n"
     ]
    }
   ],
   "source": [
    "# We need only 'id','d','sales' to make lags and rollings\n",
    "df = df[['id','calday','fact']]\n",
    "SHIFT_DAY = 28  # 28 if calday\n",
    "\n",
    "# Lags with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create lags')\n",
    "\n",
    "LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\n",
    "df = df.assign(**{\n",
    "        '{}_lag_{}'.format(col, l): df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
    "        for l in LAG_DAYS\n",
    "        for col in [TARGET]\n",
    "    })\n",
    "\n",
    "# Minify lag columns\n",
    "for col in list(df):\n",
    "    if 'lag' in col:\n",
    "        df[col] = df[col].astype(np.float16)\n",
    "\n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "# Rollings with 28 day shift\n",
    "start_time = time.time()\n",
    "print('Create rolling aggs')\n",
    "\n",
    "# for days\n",
    "for i in [7,14,30,60,180]:\n",
    "    print('Rolling period:', i)\n",
    "    df['rolling_mean_'+str(i)] = df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n",
    "    df['rolling_std_'+str(i)]  = df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n",
    "\n",
    "# Rollings with sliding shift\n",
    "for d_shift in [1,7,14]: \n",
    "    print('Shifting period:', d_shift)\n",
    "    for d_window in [7,14,30,60]:\n",
    "        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)\n",
    "        df[col_name] = df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)\n",
    "    \n",
    "print('%0.2f min: Lags' % ((time.time() - start_time) / 60))\n",
    "\n",
    "df.to_pickle(processed_data_dir+'1. lags_df_'+str(SHIFT_DAY)+'.pkl')\n",
    "df.to_excel(processed_data_dir+'1. lags_df_'+str(SHIFT_DAY)+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff894fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year_iso', 'month', 'week_iso', 'calday', 'day', 'dayofweek',\n",
       "       'calday_time', 'Holiday_type', 'region_num', 'format', 'plant',\n",
       "       'group_id', 'cat_id', 'subcat_id', 'rt_promoct_unit_ABC', 'id',\n",
       "       'flag_oos', 'duration_rt_promoct_days_in_week_max',\n",
       "       'day_sales_qty_total', 'TotalSales', 'avg_sales_day_sku_plant',\n",
       "       'price_total', 'fact'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(processed_data_dir+'a. grid_part_1.pkl')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65664442",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS(['year_iso', 'week_iso', 'calday', \n",
    "         \n",
    "         'region_num', 'plant', 'group_id','cat_id', 'subcat_id', \n",
    "         'flag_oos', 'format', 'price_total', 'duration_rt_promoct_days_in_week_max', 'fact', 'day_sales_qty_total',\n",
    "         'TotalSales', 'avg_sales_day_sku_plant', \n",
    "         'Holiday_type', \n",
    "         'calday_time',\n",
    "         'day', 'dayofweek', 'month', \n",
    "         'rt_promoct_unit_ABC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8618c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['region_num']\n",
      "Encoding ['plant']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['subcat_id']\n",
      "Encoding ['region_num', 'cat_id']\n",
      "Encoding ['region_num', 'subcat_id']\n",
      "Encoding ['plant', 'cat_id']\n",
      "Encoding ['plant', 'subcat_id']\n",
      "Encoding ['rt_promoct_unit_ABC']\n",
      "Encoding ['rt_promoct_unit_ABC', 'region_num']\n",
      "Encoding ['rt_promoct_unit_ABC', 'plant']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(processed_data_dir+'a. grid_part_1.pkl')\n",
    "df['fact'][df['calday']>20230109] = np.nan\n",
    "base_cols = list(df)\n",
    "\n",
    "icols =  [\n",
    "            ['region_num'],                  ###  ['region_num']                ['state_id'],\n",
    "            ['plant'],                       ###  ['plant']                     ['store_id'],\n",
    "            # ['group_id'],\n",
    "            ['cat_id'],                      ###  ['cat_id']                    ['cat_id'],\n",
    "            ['subcat_id'],                   ###  ['subcat_id']                 ['dept_id'],\n",
    "            ['region_num', 'cat_id'],        ###  ['region_num', 'cat_id']      ['state_id', 'cat_id'],\n",
    "            ['region_num', 'subcat_id'],     ###['state_id', 'dept_id'],        ['region_num', 'subcat_id']\n",
    "            ['plant', 'cat_id'],             ###['store_id', 'cat_id'],         ['plant', 'cat_id']\n",
    "            ['plant', 'subcat_id'],          ###['store_id', 'dept_id'],        ['plant', 'subcat_id']\n",
    "            ['rt_promoct_unit_ABC'],               ###['item_id']               ['rt_promoct_unit_ABC']\n",
    "            ['rt_promoct_unit_ABC', 'region_num'], ###['item_id', 'state_id'],  ['rt_promoct_unit_ABC', 'region_num']\n",
    "            ['rt_promoct_unit_ABC', 'plant']       ###['item_id', 'store_id']   ['rt_promoct_unit_ABC', 'plant']\n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    df['enc'+col_name+'mean'] = df.groupby(col)['fact'].transform('mean').astype(np.float16)\n",
    "    df['enc'+col_name+'std'] = df.groupby(col)['fact'].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(df) if col not in base_cols]\n",
    "df = df[['id','calday']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "062bdb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "print('Save Mean/Std encoding')\n",
    "df.to_pickle(processed_data_dir+'b. mean_encoding_df.pkl')\n",
    "df.to_excel(processed_data_dir+'2. mean_encoding_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd6af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
